# Shared Mental Models: Three-Level Explanation

## Level 1: Ages 5-10

### The Soccer Team Secret

Imagine you're on a soccer team. It's the championship game, and something amazing happens: Maya kicks the ball toward the goal. Without anyone saying a word, Carlos runs to one spot, Sophia runs to another, and the goalie doesn't know where to look. They all just... knew where to go.

How did they do that?

**The Practice Secret**

Carlos and Sophia have been on Maya's team for two years. They've practiced together hundreds of times. They've seen Maya kick that exact way before. And they've learned:

- When Maya does *this*, she's going to kick *there*
- When the goalie is *here*, Sophia should go *there*
- When Carlos is *ready*, Maya should pass *now*

They don't need to talk about it. They just know.

**The Opposite Team**

Now imagine a team where everyone is new. They're all good players, but they've never played together. Maya kicks the ball the same way, but:

- Carlos doesn't know where Maya likes to kick
- Sophia doesn't know what Carlos is going to do
- Nobody knows where anyone else will be

So what happens? They have to yell instructions: "Go left! No, your other left! I'm open! Pass it NOW!"

That takes time. And by the time they figure it out, the other team has already moved.

**The Magic Understanding**

The secret of the first team isn't that they're better players. It's that they have a **shared understanding** of how the game works and how each other plays. Scientists call this a "shared mental model."

It's like everyone has the same picture in their head of what's supposed to happen next. When you all have the same picture, you don't need to talk as much. You can just play.

**The Big Lesson**

Working together isn't just about skill. It's about understanding each other so well that you can predict what your teammates will do---and they can predict you too. That takes time, practice, and paying attention to each other.

---

## Level 2: High School Graduate

### Why Expert Teams Don't Need to Talk

In 2009, Captain Chesley "Sully" Sullenberger landed a disabled Airbus A320 on the Hudson River, saving all 155 people aboard. The entire emergency lasted 208 seconds from bird strike to water landing. What made this possible wasn't just Sully's skill---it was how his crew worked together.

First Officer Jeffrey Skiles didn't need to be told what to do. When Sully announced "My aircraft" (taking control), Skiles immediately began the emergency checklist. When Sully said "We're gonna be in the Hudson," Skiles didn't argue or ask questions. He prepared for water landing. The cockpit voice recorder shows minimal communication: each crew member knew their role and executed it without extensive coordination.

This wasn't magic. It was a shared mental model---the same understanding of what was happening and what needed to happen next.

**What Shared Mental Models Actually Are**

A shared mental model is more than shared information. Two people can know the same facts but organize them differently, leading to different predictions and different actions. A shared mental model means:

1. **Shared task understanding**: What needs to happen and in what order
2. **Shared teammate understanding**: What each person knows, can do, and will likely do
3. **Shared interpretation**: What ambiguous situations mean
4. **Shared predictions**: What happens next

The key feature is **prediction**. Team members with strong shared mental models can anticipate:
- What the situation will require next
- What teammates will need
- What teammates will do

When predictions are accurate and aligned, teams coordinate implicitly---they don't need to talk because everyone already knows.

**How Shared Mental Models Develop**

Shared mental models emerge through several mechanisms:

**Shared training**: Medical teams complete the same training. Flight crews undergo the same certifications. This creates baseline shared understanding before team members ever work together.

**Shared experience**: Teams that work together repeatedly develop deeper shared models. They learn each other's tendencies, preferences, and patterns. Research on surgical teams shows that team familiarity is associated with shorter operative times, fewer errors, and better outcomes.

**Explicit briefings**: Before complex tasks, teams often brief---not just sharing information but explicitly establishing shared understanding. "Here's what we're trying to accomplish. Here's the plan. Here's what to watch for." Effective briefings don't just transmit information; they create opportunity to verify that everyone interprets it the same way.

**Cross-training**: Understanding other roles builds shared models. When a nurse understands what the surgeon sees and knows, they can better anticipate the surgeon's needs.

**After-action reviews**: Discussing what happened after the fact surfaces and reconciles mental models. "I thought you meant X." "No, I meant Y." This explicit discussion aligns models for next time.

**When Shared Mental Models Enable Implicit Coordination**

Research distinguishes two components of implicit coordination:

1. **Anticipation**: Predicting what the team needs before being asked
2. **Dynamic adjustment**: Adapting behavior based on anticipated needs without explicit communication

Under high workload, effective teams actually communicate *less* than ineffective teams. The research finding: "Under high workload conditions, effective teams switch communication strategies from explicit closed loop communication, to implicit communication while maintaining high levels of performance."

This seems counterintuitive---shouldn't teams communicate more when things get hard? But if you have to explain everything, you're using bandwidth that could go to the task. Strong shared mental models mean you don't have to explain; teammates already know.

**The Limits of Implicit Coordination**

Implicit coordination fails when:

- **Situations are novel**: If nobody has seen this before, there's no shared model for it
- **Models have diverged**: If people have updated their understanding differently, predictions will conflict
- **Stakes are high**: When errors are costly, you need verification that everyone agrees
- **Ambiguity exists**: When the situation could be interpreted multiple ways, explicit confirmation is necessary

Expert teams recognize these limits. The surgical timeout exists precisely because implicit coordination isn't enough for safety-critical verification. Even teams with excellent shared models explicitly verify patient, procedure, and site before surgery.

**The Communication Paradox**

Here's something important: observing low communication doesn't mean shared models are strong.

- Strong models + low communication = efficient implicit coordination
- Weak models + low communication = coordination failure waiting to happen

Teams with weak shared models who don't communicate much aren't efficient---they're in danger. They're each operating on different assumptions without checking alignment.

The metric isn't communication volume; it's coordination quality. Sometimes that requires more communication (when aligning models). Sometimes that requires less (when models are aligned). The skill is knowing which mode is appropriate.

**What This Means for Any Team**

1. **Invest in shared experience**: Working together repeatedly builds shared models that can't be created by documentation alone

2. **Brief explicitly**: Don't assume everyone sees the situation the same way. State your understanding; invite challenge

3. **Verify at critical points**: When stakes are high, don't trust implicit coordination. Explicitly confirm

4. **Debrief after the fact**: Surface what each person thought was happening. Find divergences and reconcile them

5. **Cross-train when possible**: Understanding other roles helps you predict their needs and behavior

The goal isn't eliminating communication---it's having shared understanding deep enough that you communicate only when you need to, and you know when you need to.

---

## Level 3: Expert

### Shared Mental Models as Infrastructure for Implicit Coordination

The surface understanding of shared mental models treats them as "team members having similar knowledge." This framing obscures the critical insight: shared mental models are not shared information but shared **interpretive frameworks** that enable team members to predict each other's behavior, anticipate needs, and coordinate implicitly.

This distinction has profound implications for system design. Data synchronization ensures teams have the same information. Shared mental models require teams to **interpret that information the same way** and **generate the same predictions** from it. These are fundamentally different engineering problems.

**Theoretical Foundations**

**Cannon-Bowers et al. (1993)** established the four-domain model of shared mental models:

1. **Equipment model**: Shared understanding of tools, technology, and systems
2. **Task model**: Shared understanding of procedures, sequences, contingencies
3. **Team interaction model**: Shared understanding of roles, responsibilities, information flow
4. **Team member model**: Shared understanding of each member's knowledge, skills, tendencies

These are often consolidated into **taskwork models** (equipment + task) and **teamwork models** (interaction + member). The research shows both are necessary: taskwork models enable understanding of what needs to happen; teamwork models enable understanding of how the team accomplishes it.

**Klimoski and Mohammed (1994)** argued that cognition can be a group-level phenomenon. The shared mental model is not simply identical copies of knowledge in each person's head; it is the *overlap and alignment* of mental representations that enables coordinated action.

**Cooke et al. (2013)** introduced Interactive Team Cognition (ITC), arguing that team cognition is not a property or product but an *activity*. This reframes the question from "Do team members have similar mental models?" to "Is the team engaging in coherent cognitive activity?"

The ITC perspective suggests that perhaps the goal is not synchronizing static knowledge representations but ensuring agents engage in coherent joint cognitive activity. Communication and coordination patterns become direct measures of team-level cognition, not just indicators of underlying mental models.

**Common Ground and Mutual Knowledge**

Herbert Clark's theory of common ground provides linguistic/cognitive foundations. Common ground is "the sum of mutual, common, or joint knowledge, beliefs, and suppositions."

Critical distinction between knowledge levels:
- **Private knowledge**: A knows X, but doesn't know whether B knows X
- **Shared knowledge**: A knows X, and knows that B knows X
- **Mutual knowledge**: A knows X, knows that B knows X, knows that B knows that A knows X... (infinite recursion)

The **Mutual Knowledge Paradox**: True mutual knowledge requires infinite recursive verification, impossible in finite time. In practice, humans use truncation heuristics:

- **Copresence heuristic**: If we both perceive something together, we both know it
- **Community membership**: If we're both surgeons, we share surgical training knowledge
- **Linguistic evidence**: If you responded appropriately, you understood me

For AI agents, these heuristics don't naturally apply. Agents may need explicit protocols to establish common ground that humans accomplish implicitly.

**The Predictive Processing Framework**

Modern cognitive science emphasizes the brain as a prediction machine. We constantly generate predictions about what will happen next and update our models when predictions fail.

In teams with strong SMMs, members' predictions about task progression and teammate behavior are:
- **Accurate**: Predictions match what actually happens
- **Aligned**: Different team members generate similar predictions
- **Efficient**: Predictions require minimal cognitive effort

When predictions fail (something unexpected happens), attention is drawn to the mismatch, triggering model updating or explicit communication.

This prediction-based coordination reduces communication requirements: team members can predict what others are doing and what information they need. They don't have to ask.

**The Efficiency-Robustness Tradeoff**

Strong shared mental models increase efficiency by enabling implicit coordination. But they may reduce robustness:

**Efficiency gains**:
- Reduced communication overhead
- Faster decision-making
- Smoother coordination
- Lower cognitive load

**Robustness risks**:
- Shared blind spots (everyone misses the same thing)
- Groupthink (dissent suppressed)
- Over-confidence (reduced verification)
- Model ossification (failure to update)

Research on cognitive biases in teams finds: "In groups with strong shared mental models, members tend to seek out information that supports their existing framework while ignoring or dismissing evidence that contradicts it."

Optimal systems balance: enough shared understanding for implicit coordination, enough diversity and verification for error detection.

**Causes of Mental Model Divergence**

**Information asymmetry**: Different team members receive different information based on role-specific access, sequential exposure, and attention allocation. Research found surgical team members agreed 87% of the time about when key tasks should be done, but only 70% about who should do them, and certain tasks had only 39% agreement.

**Differential expertise**: Team members with different training see different things, interpret the same data differently, and project different futures. The expert's model may be more accurate, but the team needs aligned models to coordinate.

**Cognitive biases**: Confirmation bias, anchoring, groupthink, and availability heuristic can cause models to diverge or become collectively incorrect.

**Communication failures**: Incomplete transmission, ambiguous encoding, failed reception, no verification. Research found communication breakdowns were the second most common factor in OR errors after technical performance.

**Dynamic environment**: Mental models become stale as situations evolve. Temporal divergence (different update rates), hidden state changes, and plan obsolescence create drift.

**Recovery Mechanisms**

**Discrepancy detection**: The primary trigger for repair is detecting that something doesn't match expectations. This requires attention to prediction failures, which busy teams may miss.

**Constructive conflict**: Explicit disagreement, properly handled, repairs divergent models. This requires psychological safety---team members must feel safe to voice disagreement.

**Closed-loop communication**: Explicit verification that messages were received and understood correctly. Sender transmits, receiver echoes back, sender confirms.

**Resynchronization events**: When divergence is suspected but not localized, teams can "resync from scratch"---returning to a known shared state and rebuilding. The surgical timeout serves this function: regardless of what anyone thinks they know, explicitly verify patient, procedure, site.

**Distributed Situation Awareness**

Stanton et al. (2006) introduced Distributed Situation Awareness (DSA), arguing that team members have "compatible, but not identical SA... we should not always hope for, or indeed want, sharing of this awareness, as different system agents have different purposes."

This is crucial: the goal is not identical models but **compatible** models. What matters is:
- Models that aren't contradictory
- Alignment on critical elements (goals, priorities, coordination requirements)
- Complementarity where specialization adds value

The anesthesiologist doesn't need the surgeon's detailed procedural model. The surgeon doesn't need the anesthesiologist's detailed pharmacology model. But both need aligned models of the patient's status, the procedure's phase, and what happens if things go wrong.

**Implications for Design**

1. **Design for interpretation alignment, not just information sharing**: Ensure agents interpret the same data the same way through shared semantics, explicit interpretation rules, and verified understanding.

2. **Build prediction infrastructure**: Agents need models of each other's behavior, not just current state. This enables anticipation, the foundation of implicit coordination.

3. **Implement verification protocols**: Don't assume shared understanding; verify it explicitly at critical points. The agent equivalent of surgical timeouts.

4. **Plan for divergence**: Mental models will drift. Build detection mechanisms and repair protocols. Assume alignment degrades and requires maintenance.

5. **Use humans for interpretation ambiguity**: When agents' interpretive frameworks are insufficient, escalate to humans who can apply richer shared frameworks.

6. **Balance implicit and explicit coordination**: Implicit coordination is efficient but fragile. Explicit coordination is robust but costly. Systems need both, selected appropriately for the situation.

7. **Invest in agent "training"**: Agents working together need to develop coordination patterns, not just individual capabilities. This may require simulated joint training before deployment.

**The Fundamental Limitation**

Current AI systems lack the cognitive infrastructure that makes human SMMs work:
- No genuine Theory of Mind
- No embodied experience grounding interpretation
- No cultural common ground
- No accumulated lifetime of shared human experience

Agent coordination must compensate through explicit design what humans accomplish implicitly. This is achievable for well-defined tasks with clear semantics. It remains challenging for open-ended tasks requiring rich interpretation.

The path forward is not to replicate human cognition in agents, but to design coordination mechanisms appropriate for agents' actual capabilities---explicit where agents are weak, leveraging agents' strengths in consistency, speed, and tirelessness.

---

## Document Metadata

**Model Used:** Claude Opus 4.5 (claude-opus-4-5-20251101)
**Created:** 2026-01-20
**Purpose:** Three-level explanation (ages 5-10, high school, expert) for cross-disciplinary mental model research

---

## Sources

### Primary Academic Sources

- Cannon-Bowers, J.A., Salas, E., & Converse, S. (1993). "Shared mental models in expert team decision making." *Individual and Group Decision Making*, 221, 221-46. The foundational framework defining four domains of shared mental models.

- Klimoski, R., & Mohammed, S. (1994). "Team mental model: Construct or metaphor?" *Journal of Management*, 20(2), 403-437. Established cognition as a group-level phenomenon.

- Endsley, M.R. (1995). "Toward a theory of situation awareness in dynamic systems." *Human Factors*, 37(1), 32-64. The canonical three-level situation awareness model.

- Cooke, N.J., Gorman, J.C., Myers, C.W., & Duran, J.L. (2013). "Interactive team cognition." *Cognitive Science*, 37(2), 255-285. Team cognition as activity rather than property.

- Rico, R., Sanchez-Manzanares, M., Gil, F., & Gibson, C. (2008). "Team implicit coordination processes: A team knowledge-based approach." *Academy of Management Review*, 33(1), 163-184. Anticipation and dynamic adjustment components.

- Stanton, N.A., et al. (2006). "Distributed situation awareness in dynamic systems." *Ergonomics*, 49(12-13), 1288-1311. Compatible rather than identical awareness.

- Clark, H.H. & Brennan, S.E. (1991). "Grounding in Communication." *Perspectives on Socially Shared Cognition.* Common ground theory foundations.

### Surgical Team Research

- BMC Medical Education (2016): "Assessing the similarity of mental models of operating room team members"
- Lingard et al.: "Communication failures in the operating room: An observational classification of recurrent types and effects"
- PMC: "Factors Influencing Team Behaviors in Surgery"
- PMC: "The impact of a daily pre-operative surgical huddle"

### Applied Sources

- National Academies: "Human-AI Teaming: State-of-the-Art and Research Needs"
- Theoretical Issues in Ergonomics Science: "The role of shared mental models in human-AI teams"
- Tufts HRI Lab: "A framework for developing and using shared mental models in human-agent teams"

### Related Documents in This Repository

- [Shared Mental Models Deep Research](./shared-mental-models.md) - Source document
- [OODA Loop Three-Level](../management/ooda-loop-three-level.md) - Template document
- [Silo Awareness](../incident-response/silo-awareness.md) - Related coordination model
